#!/bin/bash
# =============================================================================
# GPU Watchdog - Full Deployment Script
# =============================================================================
# Orchestrates the complete deployment from zero to running cluster:
#   1. Provisions RunPod infrastructure via Terraform
#   2. Waits for pods to be ready
#   3. Updates docker/.env with pod IDs
#   4. Initializes Slurm on head and worker nodes
#   5. Starts local Docker Compose services
#   6. Verifies everything is working
#
# Usage:
#   ./scripts/deploy.sh           # Full deployment
#   ./scripts/deploy.sh --skip-infra  # Skip Terraform (pods already exist)
#   ./scripts/deploy.sh --destroy     # Tear down everything
# =============================================================================

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Project root
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"

# Load environment variables
if [ -f "$PROJECT_ROOT/.env" ]; then
    export $(grep -v '^#' "$PROJECT_ROOT/.env" | xargs)
fi

log() { echo -e "${BLUE}[$(date '+%H:%M:%S')]${NC} $*"; }
success() { echo -e "${GREEN}[$(date '+%H:%M:%S')] ✓${NC} $*"; }
warn() { echo -e "${YELLOW}[$(date '+%H:%M:%S')] ⚠${NC} $*"; }
error() { echo -e "${RED}[$(date '+%H:%M:%S')] ✗${NC} $*"; exit 1; }

# =============================================================================
# Helper Functions
# =============================================================================

check_dependencies() {
    log "Checking dependencies..."
    command -v terraform >/dev/null 2>&1 || error "terraform not found"
    command -v docker >/dev/null 2>&1 || error "docker not found"
    command -v jq >/dev/null 2>&1 || error "jq not found"
    command -v curl >/dev/null 2>&1 || error "curl not found"
    [ -n "$RUNPOD_API_KEY" ] || error "RUNPOD_API_KEY not set in .env"
    success "All dependencies available"
}

get_pod_ssh_info() {
    # Query RunPod API for SSH ports
    curl -s -H "Authorization: Bearer $RUNPOD_API_KEY" "https://api.runpod.io/graphql" \
        -H "Content-Type: application/json" \
        -d '{"query": "query { myself { pods { id name runtime { ports { privatePort publicPort ip type } } } } }"}' | \
        jq -r '.data.myself.pods[] | select(.name | startswith("gpu-watchdog"))'
}

wait_for_pods() {
    log "Waiting for pods to be ready..."
    local max_attempts=60
    local attempt=0

    while [ $attempt -lt $max_attempts ]; do
        # Query API and suppress jq errors for missing/null fields
        local pods=$(get_pod_ssh_info 2>/dev/null)

        # Extract ports, handling null/missing values gracefully
        local head_port=$(echo "$pods" | jq -r 'select(.name == "gpu-watchdog-head") | .runtime.ports[]? | select(.privatePort == 22) | .publicPort // empty' 2>/dev/null | head -1)
        local worker_port=$(echo "$pods" | jq -r 'select(.name == "gpu-watchdog-worker-0") | .runtime.ports[]? | select(.privatePort == 22) | .publicPort // empty' 2>/dev/null | head -1)

        if [ -n "$head_port" ] && [ -n "$worker_port" ]; then
            echo ""  # New line after dots
            success "Pods ready - Head port: $head_port, Worker port: $worker_port"
            export HEAD_SSH_PORT="$head_port"
            export WORKER_SSH_PORT="$worker_port"
            export POD_IP=$(echo "$pods" | jq -r 'select(.name == "gpu-watchdog-head") | .runtime.ports[]? | select(.privatePort == 22) | .ip // empty' 2>/dev/null | head -1)
            return 0
        fi

        attempt=$((attempt + 1))
        echo -n "."
        sleep 5
    done

    echo ""
    error "Timeout waiting for pods to be ready"
}

update_env_file() {
    log "Updating docker/.env with pod IDs..."
    cd "$PROJECT_ROOT/terraform"

    local head_id=$(terraform output -raw head_node_id 2>/dev/null)
    local worker_id=$(terraform output -json worker_node_ids 2>/dev/null | jq -r '.[0]')

    cat > "$PROJECT_ROOT/docker/.env" << EOF
# RunPod Pod IDs - auto-generated by deploy.sh
HEAD_POD_ID=$head_id
WORKER_POD_ID=$worker_id
EOF

    success "Updated docker/.env"
    cat "$PROJECT_ROOT/docker/.env"
}

init_slurm_head() {
    log "Initializing Slurm on head node..."

    ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 \
        -p "$HEAD_SSH_PORT" "root@$POD_IP" \
        "mkdir -p /etc/slurm && export NODE_ROLE=head && export CLUSTER_NAME=gpu-watchdog && bash /pre_start.sh" 2>&1 | \
        while read line; do echo "  [head] $line"; done

    success "Head node initialized"
}

init_slurm_worker() {
    log "Initializing Slurm on worker node..."

    # Get head pod ID from terraform
    cd "$PROJECT_ROOT/terraform"
    local head_pod_id=$(terraform output -raw head_node_id 2>/dev/null)

    # Use RunPod's global networking: POD_ID.runpod.internal
    local head_addr="${head_pod_id}.runpod.internal"
    log "Head node address: $head_addr (via global networking)"

    ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 \
        -p "$WORKER_SSH_PORT" "root@$POD_IP" \
        "mkdir -p /etc/slurm && export NODE_ROLE=worker && export CLUSTER_NAME=gpu-watchdog && export HEAD_NODE_IP=$head_addr && bash /pre_start.sh" 2>&1 | \
        while read line; do echo "  [worker] $line"; done

    success "Worker node initialized"
}

verify_slurm_cluster() {
    log "Verifying Slurm cluster..."

    local nodes=$(ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 \
        -p "$HEAD_SSH_PORT" "root@$POD_IP" \
        "sinfo -N -h | wc -l" 2>/dev/null)

    if [ "$nodes" -ge 2 ]; then
        success "Slurm cluster has $nodes nodes"
        ssh -o StrictHostKeyChecking=no -p "$HEAD_SSH_PORT" "root@$POD_IP" "sinfo -N -l" 2>/dev/null
        return 0
    else
        warn "Only $nodes node(s) in cluster, expected 2"
        return 1
    fi
}

start_services() {
    log "Starting Docker Compose services..."
    cd "$PROJECT_ROOT"
    docker compose -f docker/docker-compose.yml up -d --force-recreate
    success "Services started"
}

verify_metrics() {
    log "Waiting for metrics to be available..."
    sleep 10

    local response=$(curl -s http://localhost:8080/api/metrics/current 2>/dev/null)
    local node_count=$(echo "$response" | jq '.nodes | length' 2>/dev/null)

    if [ "$node_count" -ge 2 ]; then
        success "Metrics available for $node_count nodes"
        echo "$response" | jq '{nodes: [.nodes[] | {nodeId, gpu: .gpus[0].gpuName, util: .gpus[0].utilization}], slurm: {nodes: .slurm.nodesTotal, cpus: .slurm.cpusTotal}}'
        return 0
    else
        warn "Only $node_count node(s) reporting metrics"
        return 1
    fi
}

print_summary() {
    echo ""
    echo -e "${GREEN}═══════════════════════════════════════════════════════════════${NC}"
    echo -e "${GREEN}  GPU Watchdog Deployment Complete!${NC}"
    echo -e "${GREEN}═══════════════════════════════════════════════════════════════${NC}"
    echo ""
    echo -e "  ${BLUE}Dashboard:${NC}     http://localhost:3000"
    echo -e "  ${BLUE}Prometheus:${NC}    http://localhost:9090"
    echo -e "  ${BLUE}Grafana:${NC}       http://localhost:3001 (admin/gpuwatchdog)"
    echo ""
    echo -e "  ${BLUE}SSH Commands:${NC}"
    echo -e "    Head:   ssh root@$POD_IP -p $HEAD_SSH_PORT"
    echo -e "    Worker: ssh root@$POD_IP -p $WORKER_SSH_PORT"
    echo ""
    echo -e "  ${BLUE}Slurm Commands (on head):${NC}"
    echo -e "    sinfo -N -l           # List nodes"
    echo -e "    squeue                # List jobs"
    echo -e "    sbatch --wrap='...'   # Submit job"
    echo ""
    echo -e "  ${YELLOW}Remember: make infra-destroy when done to save costs!${NC}"
    echo ""
}

destroy_all() {
    log "Destroying all resources..."

    log "Stopping Docker Compose services..."
    cd "$PROJECT_ROOT"
    docker compose -f docker/docker-compose.yml down 2>/dev/null || true

    log "Destroying Terraform infrastructure..."
    cd "$PROJECT_ROOT/terraform"
    terraform destroy -auto-approve

    success "All resources destroyed"
}

# =============================================================================
# Main
# =============================================================================

main() {
    echo ""
    echo -e "${BLUE}═══════════════════════════════════════════════════════════════${NC}"
    echo -e "${BLUE}  GPU Watchdog - Full Deployment${NC}"
    echo -e "${BLUE}═══════════════════════════════════════════════════════════════${NC}"
    echo ""

    # Parse arguments
    local skip_infra=false
    for arg in "$@"; do
        case $arg in
            --skip-infra)
                skip_infra=true
                ;;
            --destroy)
                destroy_all
                exit 0
                ;;
            --help|-h)
                echo "Usage: $0 [OPTIONS]"
                echo ""
                echo "Options:"
                echo "  --skip-infra  Skip Terraform provisioning (use existing pods)"
                echo "  --destroy     Tear down all infrastructure"
                echo "  --help        Show this help message"
                exit 0
                ;;
        esac
    done

    cd "$PROJECT_ROOT"
    check_dependencies

    # Step 1: Terraform
    if [ "$skip_infra" = false ]; then
        log "Provisioning infrastructure via Terraform..."
        cd "$PROJECT_ROOT/terraform"
        terraform init -upgrade >/dev/null
        terraform apply -auto-approve
        success "Infrastructure provisioned"
    else
        warn "Skipping infrastructure provisioning (--skip-infra)"
    fi

    # Step 2: Wait for pods
    wait_for_pods

    # Step 3: Update env file
    update_env_file

    # Step 4: Initialize Slurm
    log "Initializing Slurm cluster..."
    init_slurm_head
    sleep 5
    init_slurm_worker
    sleep 5
    verify_slurm_cluster || warn "Cluster verification had issues, continuing..."

    # Step 5: Start services
    start_services

    # Step 6: Verify
    verify_metrics || warn "Metrics verification had issues, check dashboard manually"

    # Summary
    print_summary
}

main "$@"
